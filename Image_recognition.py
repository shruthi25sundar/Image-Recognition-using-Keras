# -*- coding: utf-8 -*-
"""

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vLW74L0KTVRSt6qCR_W8QkACNjZAfi7l
"""

from keras.models import Model, Sequential
from keras.optimizers import Adam
from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D, ZeroPadding2D,Conv2D
from keras.layers import Input, Lambda, Activation
from keras.utils import np_utils
from keras.datasets import mnist
from keras import backend as K
import keras
import keras.utils
from keras import utils as np_utils
import pandas as pd
from numpy import genfromtxt
 
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import clear_output
from collections import defaultdict
from sklearn.datasets import fetch_mldata
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_openml

from google.colab import drive
drive.mount('/content/gdrive')  #Mount gdrive to load the datasets

# load and shape data as usual, but here we don't process class labels
# to one-hot encoding. In fact, we don't exactly use class labels
# during training, only while setting up the triplets.
(X_train, y_train), (X_test, y_test) = mnist.load_data()


X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255

def get_image(label, test=False):
    
    """Choose an image from our training or test data with the
    given label."""
    if test:
        y = y_test; X = X_test
    else:
        y = y_train; X = X_train
    idx = np.random.randint(len(y))
    while y[idx] != label:
        # keep searching randomly!
        idx = np.random.randint(len(y))
    return X[idx]

def get_triplet(test=False):
    """Choose a triplet (anchor, positive, negative) of images
    such that anchor and positive have the same label and
    anchor and negative have different labels."""
    n = a = np.random.randint(10)
    while n == a:
        # keep searching randomly!
        n = np.random.randint(10)
    a, p = get_image(a, test), get_image(a, test)
    n = get_image(n, test)
    return a, p, n

def generate_triplets(test=False):
    """Generate an un-ending stream (ie a generator) of triplets for
    training or test."""
    while True:
        list_a = []
        list_p = []
        list_n = []

        for i in range(batch_size):
            a, p, n = get_triplet(test)
            list_a.append(a)
            list_p.append(p)
            list_n.append(n)
            
        A = np.array(list_a, dtype='float32')
        P = np.array(list_p, dtype='float32')
        N = np.array(list_n, dtype='float32')
        # a "dummy" label which will come in to our identity loss
        # function below as y_true. We'll ignore it.
        label = np.ones(batch_size) 
        yield [A, P, N], label

############## Loss ###########################

def identity_loss(y_true, y_pred):
    
    """This loss function just takes the mean of y_pred. Because of the
    way we wire the network (see complete_model below), y_pred is the
    output of the triplet loss, so minimising it is what we want to
    do."""
    return K.mean(y_pred)

def triplet_loss(x):
    """The triplet loss is ||A - P|| - ||A - N|| + alpha, where ||.||
    is the Euclidean norm. Notice that this is not a loss function in the
    format expected by Keras, ie f(y_true, y_pred)."""
    anchor, positive, negative = x

    

    # distance between the anchor and the positive
    pos_dist = K.sum(K.square(anchor-positive),axis=1)

#     # distance between the anchor and the negative
    neg_dist = K.sum(K.square(anchor-negative),axis=1)

#     # compute loss
    alpha=0.2
    basic_loss =( pos_dist-neg_dist)+alpha
    loss = K.maximum(basic_loss,0.0)
  
    return loss

def embedding_model():
    input_shape=(28,28,1)
    #various layers of embedding
    base_input = Input(input_shape)
    model = Sequential()
    model.add(Conv2D(128, (5, 5), input_shape=(28, 28, 1), activation='relu'))   #Changing the dimensions and using relu function
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Conv2D(128, (5, 5), input_shape=(28, 28, 1), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dense(embedding_dim, name = "embedding"))
    
   
    return model

def complete_model(base_model):
    """This part of the model is quite tricky. Rather than a Sequential
    model, we declare a Model and say which are its inputs and
    outputs, and we declare how the outputs are calculated from the
    inputs. In particular, there are no layers in this model, *other
    than* the layers in the embedding model discussed above.

    A further complication is that our triplet loss can't be
    calculated as a function of y_true and y_predicted as
    usual. Instead we calculate the triplet loss as an extra Lambda
    layer. Then the Model's loss is set to be equal to the triplet
    loss via the identity function."""
    #input dataset
    
    input_1 = Input((imsize, imsize, 1)) 
    input_2 = Input((imsize, imsize, 1))
    input_3 = Input((imsize, imsize, 1))

    # call the base_model three times to get A, P, N

    # XXX YOUR CODE HERE.
    #Anchor ,positive and negative is embedded for the given input 
    A=base_model(input_1)  #parsed to embedding _model()
    P=base_model(input_2)
    N=base_model(input_3)
    
    
    loss = Lambda(triplet_loss)([A, P, N])  #triplet loss is determined
    model = Model(inputs=[input_1, input_2, input_3], outputs=loss) #Model for the given image is generated
    #model.compile(loss=identity_loss, optimizer=Adam(LR))
    model.compile(loss=identity_loss, optimizer=Adam(LR)) #compiled 
    return model

############## Settings ###########################

imsize = 28

# XXX you might like to play with some of these hyperparameters
batch_size = 100
# 2D is interesting for visualisation, but higher allows more "space"
# to achieve accuracy in complex domains, eg 128 is common for
# faces. but MNIST is simple, so maybe 2 is enough for us anyway.
embedding_dim = 2 
LR = 0.0001 # be careful: too large will be unstable for our data
EPOCHS = 5
alpha = 0.2 # interesting to think about different values

############## Main ###############################

# create the data generators
train_generator = generate_triplets()
test_generator = generate_triplets(test=True)

# instantiate the model and take a look

# XXX YOUR CODE HERE: create the embedding model and then use
# that to create the complete model
base_model=embedding_model()
model=complete_model(base_model)    
print(model.summary())

# fit
data_model=model.fit_generator(train_generator,batch_size,EPOCHS) #the model is fit and their loss is determined at corresponding epoch
# XXX YOUR CODE HERE: call fit_generator() to fit the model.

input_4=Input((imsize, imsize, 1))       #seperate input is given
output = base_model(input_4)             #repeated the steps 
new_model = Model(inputs = input_4, outputs = output)
new = new_model.predict(X_train)         #predicted using X_train
y_train_new = keras.utils.to_categorical(y_train, len(np.unique(y_train)))   #one hot encoding
model_1 = Sequential()
model_1.add(Dense(32, activation='relu',input_shape=(2,)))
model_1.add(Dense(len(np.unique(y_train)), activation='softmax'))    #final layer of embedding is softmax function. 
model_1.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])


model_1.fit(new,y_train_new,batch_size=20,epochs=20) #model is fit and their accuracy is predicted

N = np.arange(0, EPOCHS)
plt.style.use("ggplot")
plt.figure()
plt.plot(N, data_model.history["loss"], label="train_loss")
plt.title("Training Loss")
plt.xlabel("Epoch #")
plt.ylabel("Loss") #plot for the loss is generated

#Load the datasets 
#Load emnist train images
!cp "/content/gdrive/My Drive/emnist_train_images_3.npy" "emnist_train_images_3.npy"
#Load emnist train letters
!cp "/content/gdrive/My Drive/emnist-letters-train.csv" "emnist-letters-train.csv"
#Load emnist test letters
!cp "/content/gdrive/My Drive/emnist-letters-test.csv" "emnist-letters-test.csv"

X_train_letters = genfromtxt('emnist-letters-train.csv', delimiter=',')   #The dataset is stored in letters
X_test_letters = genfromtxt('emnist-letters-test.csv', delimiter=',')

X_train_letters

X_test_letters

X_train = X_train_letters
X_train = np.delete(X_train, (0), axis=1)   #removing the first element of array

X_train = X_train.reshape(88800, 28, 28)

y_train = np.ones((88800,))*10

X_train_new=[]
for i in X_train:
  X_train_new.append(np.rot90(np.flip(i,0),1/2))   #image is roated 90 degree and flipped half to recognise the image correctly

(X_train_new_letters, y_train_new_letters), (X_test_new_letters, y_test_new_letters) = mnist.load_data() #Load the dataset

#First 30000 is considered
X_train_new_letters = X_train_new_letters[0:30000,:]   
y_train_new_letters = y_train_new_letters[0:30000] 
#First 5000
X_test_new_letters = X_test_new_letters[0:5000,:]
y_test_new_letters = y_test_new_letters[0:5000]
#Merging 
X_train = np.concatenate((X_train_new, X_train_new_letters))
y_train = np.concatenate((y_train, y_train_new_letters))


X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
X_train = X_train.astype('float32')
X_train /= 255
print(X_train.shape)



# create the data generators
train_generator = generate_triplets()
test_generator = generate_triplets(test=True)
train_next = next(train_generator)      #next iterator of train_generator

# instantiate the model and take a look

# XXX YOUR CODE HERE: create the embedding model and then use
model = complete_model(embedding_model())
# that to create the complete model
print(model.summary())

# fit
# XXX YOUR CODE HERE: call fit_generator() to fit the model.
data_model = model.fit_generator(train_generator, batch_size, EPOCHS)

input_5=Input((imsize, imsize, 1))
output = base_model(input_5)
new_model = Model(inputs = input_5, outputs = output)
new = new_model.predict(X_train)
y_train_new = keras.utils.to_categorical(y_train, len(np.unique(y_train)))
model_new= Sequential()
model_new.add(Dense(32, activation='relu',input_shape=(2,)))
model_new.add(Dense(len(np.unique(y_train)), activation='softmax')) #final layer of the embedding
model_new.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])


model_new.fit(new,y_train_new,batch_size=20,epochs=20)

#Load test image
test_emnist= np.load('emnist_train_images_3.npy')
emnist_train_image = new_model.predict(test_emnist[1].reshape(1,28,28,1))  
#the model is predicted using the given images
model_new.predict(emnist_train_image)
#the predicted letter is plotted
plt.imshow(test_emnist[1].reshape(28,28))

#Part 2



#Constructing VGG Face model 
model = Sequential()
model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, (3, 3), activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))
 
model.add(Convolution2D(4096, (7, 7), activation='relu'))
model.add(Dropout(0.5))
model.add(Convolution2D(4096, (1, 1), activation='relu'))
model.add(Dropout(0.5))
model.add(Convolution2D(2622, (1, 1)))
model.add(Flatten())
model.add(Activation('softmax'))

from keras.models import model_from_json
from keras.models import Sequential, load_model
#!ls "/content/gdrive/My Drive"
!cp "/content/gdrive/My Drive/vgg_face_weights.h5" "vgg_face_weights.h5"
model.load_weights('vgg_face_weights.h5')   #Loading pre-trained weights

#Loading downloaded images:

!cp "/content/gdrive/My Drive/1_1.jpg" "1_1.jpg"
!cp "/content/gdrive/My Drive/1_3.jpg"  "1_3.jpg"
!cp "/content/gdrive/My Drive/1_2.jpg"  "1_2.jpg"
!cp "/content/gdrive/My Drive/1_4.jpg"  "1_4.jpg"
!cp "/content/gdrive/My Drive/1_5.jpg"  "1_5.jpg"
!cp "/content/gdrive/My Drive/2_1.jpg" "2_1.jpg"
!cp "/content/gdrive/My Drive/2_3.jpg"  "2_3.jpg"
!cp "/content/gdrive/My Drive/2_2.jpg"  "2_2.jpg"
!cp "/content/gdrive/My Drive/2_4.jpg"  "2_4.jpg"
!cp "/content/gdrive/My Drive/2_5.jpg"  "2_5.jpg"
!cp "/content/gdrive/My Drive/3_1.jpg" "3_1.jpg"
!cp "/content/gdrive/My Drive/3_3.jpg"  "3_3.jpg"
!cp "/content/gdrive/My Drive/3_2.jpg"  "3_2.jpg"
!cp "/content/gdrive/My Drive/3_4.jpg"  "3_4.jpg"
!cp "/content/gdrive/My Drive/3_5.jpg"  "3_5.jpg"

#Output Layer Representation
vgg_face_descriptor = Model(inputs=model.layers[0].input
, outputs=model.layers[-2].output)

#Preprocessing of Images
from keras.applications.resnet50 import ResNet50
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input
def preprocess_image(image_path):
    img = image.load_img(image_path, target_size=(224, 224))
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    img = preprocess_input(img)
    return img

#Similarity Calculation using Cosine similarity distance
def findCosineDistance(source, test):
    image = np.matmul(np.transpose(source), test)
    first = np.sum(np.multiply(source, source))
    second = np.sum(np.multiply(test, test))
    return 1 - (image / (np.sqrt(first) * np.sqrt(second)))

#epsilon value states threshold
epsilon = 0.40 #cosine similarity

#Recognizing of images
def verifyFace(img1, img2):
  img1_representation = vgg_face_descriptor.predict(preprocess_image(img1))[0,:]
  img2_representation = vgg_face_descriptor.predict(preprocess_image(img2))[0,:]
  
  cosine_similarity = findCosineDistance(img1_representation, img2_representation)
  if(cosine_similarity < epsilon):
    print("verified... they are same person")
  else:
    print("unverified! they are not same person!")
    
  f = plt.figure()
  f.add_subplot(1,2, 1)
  plt.imshow(image.load_img(img1))
  plt.xticks([]); plt.yticks([])
  f.add_subplot(1,2, 2)
  plt.imshow(image.load_img(img2))
  plt.xticks([]); plt.yticks([])
  plt.show(block=True)
  print("-----------------------------------------")

#Loading and tesing- Test images

verifyFace('1_1.jpg','1_2.jpg')
verifyFace('1_2.jpg','1_3.jpg')
verifyFace('1_3.jpg','1_4.jpg')
verifyFace('1_4.jpg','1_5.jpg')

#Accessing images for clustering(15 images of 3 different actors i.e, 5 each)
X = []
y = []

for j in range(3):
  for i in range(5):
    X.append(vgg_face_descriptor.predict(preprocess_image(str(j+1)+"_"+str(i+1)+".jpg"))[0,:])

#KMeans clustering of similar images
from sklearn.cluster import KMeans   
kmeans = KMeans(n_clusters=3)
kmeans.fit(X)   
y_kmeans = kmeans.predict(X)

#Summarising and visualizing clusters of similar images
from sklearn.decomposition import PCA
import pylab as pl
train_img = PCA(n_components=2).fit(X)    #2d graph

pca_2d = train_img.transform(X)
pl.scatter(pca_2d[:, 0], pca_2d[:, 1], c=kmeans.labels_)  #scatterplot for the determined labels
pl.show()

#Visualizing image clusters in the scatter plot.
import matplotlib.pyplot as plt
from matplotlib.offsetbox import (OffsetImage, AnnotationBbox)

fig = plt.gcf()
fig.clf()
ax=plt.subplot()
ax.grid(True)


ax.set_ylim(bottom=-1,top=1)  #setting y axis limit
ax.set_xlim(right=1,left=-1)  #setting x axis limit

imglist=["1_1.jpg","1_2.jpg","1_3.jpg","1_4.jpg","1_5.jpg","2_1.jpg","2_2.jpg","2_3.jpg","2_4.jpg","2_5.jpg","3_1.jpg","3_2.jpg","3_3.jpg","3_4.jpg","3_5.jpg"]
i=0
for j in imglist:
  arr_img = plt.imread(j)      #reading the image
  imagebox = OffsetImage(arr_img, zoom=.2) #zoom size
  xy=[pca_2d[:,0][i]/100,pca_2d[:,1][i]/100] # dividing the scale by 100 
  ab=AnnotationBbox(imagebox,xy)    #plotting the image in the window
  ax.add_artist(ab)
  i=i+1                             #incrementing to access every image and plot
